---
layout: home
author_profile: true
classes: wide
---

<p>
    Hello! I'm an aspiring PhD student looking to study NLP. Currently I'm at <a href="https://www2.brockport.edu/">SUNY Brockport</a> studying Math and Computer Science. I've spent a lot of time recently thinking about how to make language models more efficient and robust. Some topics of interest to me are: language model evaluation metrics, quantization, memory efficient training techniques, and applicaations of NLP to other fields. 
    <br>
    <br>
    If you have any suggestions for books or papers to read, please let me know! I'm always looking for new things to learn. My current reading list contains:
    <ul>
        <li><a href="https://arxiv.org/pdf/2106.09685.pdf">arXiv</a> - (Deep Dive) LoRA: Low-Rank Adaptation of Large Language Models</li>
        <li><a href="https://arxiv.org/abs/2311.11045">arXiv</a> - Orca 2: Teaching Small Language Models How to Reason</li>
        <li><a href="https://arxiv.org/abs/2309.05463">arXiv</a> - Textbooks Are All You Need II: phi-1.5 technical report</li>
        <li><a href="https://github.com/RoundtableML/ML-Fundamentals-Reading-Lists">github</a> - (Many) ML Fundamentals Reading Lists</li>
    </ul>

    <!-- Things I've already read-->
    <details>
        <summary>Past Reads</summary>
        <br>I'm new yet! I've only read a few papers so far:
        <ul>
            <li><a href="https://arxiv.org/abs/2303.08774">arXiv</a> - GPT-4 Technical Report</li>
        </ul>
    </details>

    <hr>
</p>

