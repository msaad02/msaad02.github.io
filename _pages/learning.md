---
layout: home
title:  "Learning"
permalink: /learning/
author_profile: true
classes: wide
---

For winter break 2023, I'm working on the following:

1. **Finish my honors thesis**, BrockportGPT, and work on getting it published (see Projects tab)
2. Finish [Advent of Code 2023](https://adventofcode.com/2023/)! Check out my solutions [here](https://github.com/msaad02/AdventOfCode2023)

---

Some things I'd like to learn/am working on learning:

- Differential programming ([pytorch](https://pytorch.org/) and [jax](https://jax.readthedocs.io/en/latest/index.html) mainly)
- Memory efficient techniques for training and inference of LMs (LoRA, GPTQ, etc.) in the math sense
- How to use [gradio](https://www.gradio.app/) to make interactive demos of my models
- Building out this website
- Open model experimentation (would like to try out Mixtral, Phi-1.5, and Orca 2 specifically)

---

My current reading list contains:
<ul>
    <li><a href="https://arxiv.org/pdf/2106.09685.pdf">arXiv</a> - (Deep Dive) LoRA: Low-Rank Adaptation of Large Language Models</li>
    <li><a href="https://arxiv.org/abs/2311.11045">arXiv</a> - Orca 2: Teaching Small Language Models How to Reason</li>
    <li><a href="https://arxiv.org/abs/2309.05463">arXiv</a> - Textbooks Are All You Need II: phi-1.5 technical report</li>
    <li><a href="https://github.com/RoundtableML/ML-Fundamentals-Reading-Lists">github</a> - (Many) ML Fundamentals Reading Lists</li>
</ul>

<!-- Things I've already read-->
<details>
    <summary>Things I've Read</summary>
    <ul>
        <li><a href="https://arxiv.org/abs/2303.08774">arXiv</a> - GPT-4 Technical Report</li>
    </ul>
</details>
